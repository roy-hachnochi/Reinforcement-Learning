{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Connect4 - Deep Q-Learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e42c27bfdaa3400aa4f25b5980739276":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a413e648be9d4ef0a81d16509ec6be14","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d0c808377bbe4af190a2b1ae7984ca89","IPY_MODEL_d2e9701cd9214782b11325ad1117ba83"]}},"a413e648be9d4ef0a81d16509ec6be14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0c808377bbe4af190a2b1ae7984ca89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_da70645db5e14ddf867d0602dab26ac3","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"","max":200000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":445,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_69fb94ac54a24fcaa5cfaf035c34ddf5"}},"d2e9701cd9214782b11325ad1117ba83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6bff33f24d85444d9ee881bdafce5c2a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 445/200000 [00:06&lt;51:47, 64.22it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_480cd399272b4f6e853c54b0f1416f84"}},"da70645db5e14ddf867d0602dab26ac3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"69fb94ac54a24fcaa5cfaf035c34ddf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6bff33f24d85444d9ee881bdafce5c2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"480cd399272b4f6e853c54b0f1416f84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"rA9v5PE9tF_n"},"source":["# **Reinforecement Learning - Connect4**\n","\n","\n","### Deep Q-learning\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"i3NksWTEHsX_"},"source":["We'll now use all our gained knowledge from the TicTacToe experiments in another game: Connect4!  \n","\n","TicTacToe is a pretty simple game, with a small state space, and with very little strategy. Connect4 is a bit harder (still not as difficult as Chess\\Checkers\\Go\\Backgammon...).  \n","For the RL agent, we'll use the deep Q-learning player previously seen. Why so when we saw that the tabular Q-learning player achieved better results in TicTacToe? Because even for a simple game as Connect4, the Q-table is way too large to handle with. This has two effects:\n","- Big space complexity for the player - it must save a table containing Q-values for each state-action pair.\n","- Sparsity of the samples - due to the big state space, each state will be visited a small amount of times during training, so that our player won't be able to collect enough experience to learn the actual state-action value.\n","\n","The strength of deep Q-learning comes in handy with solving these two problems:\n","- The space complexity of the model is constant - we're not saving a value for each state-action pair, but rather using a fixed amount of weights to approximate this value. This makes the learning process scalable.\n","- Since we're using a convolutional network and our input is actually an image of the board, we're hoping that the network may learn to generalize states that it has seen and learnt and deduce into states that it hasn't seen (or has but not enough).  \n","\n","But enough talk, let's try and see if all of the above actually works."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ho0uwJzxtF_u","executionInfo":{"status":"ok","timestamp":1614326262509,"user_tz":-120,"elapsed":23838,"user":{"displayName":"Roy Hachnochi","photoUrl":"","userId":"05428385164692874905"}},"outputId":"0c80f38f-be26-4737-c20a-036ec8263ee1"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd ./drive/My\\ Drive/RL"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/RL\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9h7bkgYxVhsD","executionInfo":{"status":"ok","timestamp":1614326275188,"user_tz":-120,"elapsed":9564,"user":{"displayName":"Roy Hachnochi","photoUrl":"","userId":"05428385164692874905"}}},"source":["from Connect4.Connect4 import Connect4State, N_ROWS, N_COLS, N_CH\n","from Players.HumanPlayer import HumanPlayer\n","from Players.MaxminPlayer import MaxminPlayer\n","from Players.DQPlayer import DQPlayer, DQNArgs\n","from Players.RandomPlayer import RandomPlayer\n","from Players.SemiRandomPlayer import SemiRandomPlayer\n","from Utility.Auxiliary import *\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dn62_e0_tGAs"},"source":["---\n","## **Learning**"]},{"cell_type":"markdown","metadata":{"id":"yBaCPR7VLXxI"},"source":["We'll train our Deep Q-learning player in the methods that achieved the best results for TicTacToe: Semi-random and Self-play.   \n","For evaluation, as before, we'll test our player against 3 other players:\n","- 10,000 games against a random player.\n","- 10 games against a random minimax player, which randomly chooses between all the best actions for *k* turns ahead. We'll use *k=5* because this is empirically a very good player - it took me about 20-30 games to actually manage to win it, all of the others I lost. The down-side is that *k=5* is very time consumable, so we'll have to evaluate only 10 games.\n","- 2 games against himself.\n","\n","We'll make ourselves a convinient evaluation function for the above, which will take advantage of the ```evaluate``` function."]},{"cell_type":"code","metadata":{"id":"rcCSOm6wGUke","executionInfo":{"status":"ok","timestamp":1614326280882,"user_tz":-120,"elapsed":668,"user":{"displayName":"Roy Hachnochi","photoUrl":"","userId":"05428385164692874905"}}},"source":["state = Connect4State()\n","\n","# Evaluation function:\n","def multi_evaluation(state, player, opponents, nGamesList):\n","    N = len(opponents)\n","    pWin1 = [0] * N\n","    pWin2 = [0] * N\n","    pTie = [0] * N\n","    for i, (opp, nGames) in enumerate(zip(opponents, nGamesList)):\n","        pWin1[i], pWin2[i], pTie[i] = evaluate(state, player, opp, nGames)\n","    print()\n","    for i, (opp, nGames) in enumerate(zip(opponents, nGamesList)):\n","        print(\"{} vs. {}: P1 Win {:.2f}% | P2 Win {:.2f}% | Tie {:.2f}%\".format(player.name, opp.name, pWin1[i] * 100,\n","                                                                                pWin2[i] * 100, pTie[i] * 100))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TAysFD-R5hT"},"source":["We'll initialize some training hyperparameters, some for the neural network itself, some for the training process, and some for the MDP:"]},{"cell_type":"code","metadata":{"id":"BoQGfk62R6J9","executionInfo":{"status":"ok","timestamp":1614326426854,"user_tz":-120,"elapsed":550,"user":{"displayName":"Roy Hachnochi","photoUrl":"","userId":"05428385164692874905"}}},"source":["connect4_dqn_args = DQNArgs(ch=N_CH,                            # NN input channels - don't change this!\r\n","                            h=N_ROWS,                           # NN input height - don't change this!\r\n","                            w=N_COLS,                           # NN input wodth - don't change this!\r\n","                            output_size=N_COLS,                 # NN output size (number of actions) - don't change this!\r\n","                            layer_channels=[32, 32, 16],        # number of channels for each layer\r\n","                            layer_sizes=[5, 3, 3],              # kernel sizes for each layer\r\n","                            layer_strides=[1, 1, 1],            # stride for each layer\r\n","                            layer_padding=[0, 1, 1],            # padding for each layer\r\n","                            batch_size=32,                      # batch size for optimization step\r\n","                            mem_size=100000,                    # replay memory size\r\n","                            target_update=5000,                 # training step period for target network update\r\n","                            eps_decay=2e4,                      # exploration rate decay\r\n","                            lr=0.001,                           # learning rate\r\n","                            gamma=0.998)                        # MDP discount factor"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sX1qddSyyngU"},"source":["### Learning against a semi-random player:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["e42c27bfdaa3400aa4f25b5980739276","a413e648be9d4ef0a81d16509ec6be14","d0c808377bbe4af190a2b1ae7984ca89","d2e9701cd9214782b11325ad1117ba83","da70645db5e14ddf867d0602dab26ac3","69fb94ac54a24fcaa5cfaf035c34ddf5","6bff33f24d85444d9ee881bdafce5c2a","480cd399272b4f6e853c54b0f1416f84"]},"id":"rv8kOKgbyjaT","outputId":"9159a0db-4e63-4d1b-8365-78f82edc9e5e"},"source":["policy_fileName = \"./Connect4/DQpolicy_vsSemiRand\"\n","nGames = 200000\n","\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=True)\n","p2 = SemiRandomPlayer(\"Random\")\n","\n","log = train(state, p1, p2, nGames=nGames)\n","plot_log(log)\n","p1.save_policy(policy_fileName)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e42c27bfdaa3400aa4f25b5980739276","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"XVvMablMk3mb"},"source":["The training process does look good, but notice that the win rate platoed at around 80%. Though we should remember that there is always a small exploration rate (even at the end of the training), this still seems an unsatisfying player."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"rK5Zmde1z6qM","executionInfo":{"elapsed":966,"status":"error","timestamp":1612613079321,"user":{"displayName":"Roy Hachnochi","photoUrl":"","userId":"05428385164692874905"},"user_tz":-120},"outputId":"be83e465-016a-4c0a-835a-c0f2109eefdc"},"source":["policy_fileName = \"./Connect4/DQpolicy_vsSemiRand\"\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=False)\n","p1.load_policy(policy_fileName)\n","\n","p2 = SemiRandomPlayer(\"Semi-Random\")\n","p3 = MaxminPlayer(\"Minimax\", max_depth=5)\n","p4 = DQPlayer(\"DQ2\", connect4_dqn_args, isLearning=False)\n","p4.load_policy(policy_fileName)\n","opponents = [p2, p3, p4]\n","nGamesList = [10000, 10, 2]\n","multi_evaluation(state, p1, opponents, nGamesList)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-555cbea160df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpolicy_fileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Connect4/DQpolicy_vsSemiRand\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DQ\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnect4_dqn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misLearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_fileName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemiRandomPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Semi-Random\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DQPlayer' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Sdb75g_BOPyM"},"source":["Not looking good enough... No wins against the Minimax player, and almost 16% losses against a semi-random, which indicates that our player did learn how to try and win - but didn't get to the point of making a good look-ahead strategy."]},{"cell_type":"markdown","metadata":{"id":"7kKp_vijdE9m"},"source":["### Learning against himself (self-play):"]},{"cell_type":"code","metadata":{"id":"ZN9jynTEdIo1"},"source":["policy_fileName = \"./Connect4/DQpolicy_selfPlay\"\n","nGames = 200000\n","\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=True)\n","\n","log = train(state, p1, p1, nGames=nGames)\n","plot_log(log)\n","p1.save_policy(policy_fileName)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QD_oO951uVTZ"},"source":["I'm actually not so sure on how to read this graph. When dealing with the game of TicTacToe it was clear that our aim is to have the Tie curve grow during the training - bacause it is well-known that two perfect players will always end in a tie in TicTacToe. But in Connect4 this is not necessarily the situation, so maybe this could be good. Though I will note that a graph which bairly changes for 200,000 games shouldn't bear good news."]},{"cell_type":"markdown","metadata":{"id":"BIq_U8Z_fDlI"},"source":["Now let's test our player..."]},{"cell_type":"code","metadata":{"id":"wSofkRP2dzo1"},"source":["policy_fileName = \"./Connect4/DQpolicy_selfPlay\"\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=False)\n","p1.load_policy(policy_fileName)\n","\n","p2 = SemiRandomPlayer(\"Semi-Random\")\n","p3 = MaxminPlayer(\"Minimax\", max_depth=5)\n","p4 = DQPlayer(\"DQ2\", connect4_dqn_args, isLearning=False)\n","p4.load_policy(policy_fileName)\n","opponents = [p2, p3, p4]\n","nGamesList = [10000, 10, 2]\n","multi_evaluation(state, p1, opponents, nGamesList)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cL3b6chI1rrb"},"source":["So it seems as though the self-play training achieved fairly similar results to the semi-random, although slightly worse."]},{"cell_type":"markdown","metadata":{"id":"YI0r8XvvMdO6"},"source":["### Continued Learning:"]},{"cell_type":"markdown","metadata":{"id":"c8Z0ZxHPMeih"},"source":["We saw that both of our training methods didn't manage to achieve satisfying results. Of course it could always be due to non-tuned hyperparameters, or due to the over/under-complexity of the strucure of our Q-network, but the simplest thing to try and test is to just give more training steps - because perhaps our player was simply not given enough experience to learn from.  \n","So what we'll do is load one of the previously trained networks, and start another training process from these weights."]},{"cell_type":"code","metadata":{"id":"QRhxZuA1MfRn"},"source":["old_policy_fileName = \"./Connect4/DQpolicy_vsSemiRand\"\n","new_policy_fileName = \"./Connect4/DQpolicy_vsSemiRand2\"\n","nGames = 200000\n","\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=True)\n","p1.load_policy(old_policy_fileName)\n","p2 = SemiRandomPlayer(\"Random\")\n","\n","log = train(state, p1, p2, nGames=nGames)\n","plot_log(log)\n","p1.save_policy(new_policy_fileName)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQ49vgRjMhWe"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"LH2OETVlMtne"},"source":["Now let's test our player..."]},{"cell_type":"code","metadata":{"id":"j7flHpSGMhur"},"source":["policy_fileName = \"./Connect4/DQpolicy_vsSemiRand2\"\n","p1 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=False)\n","p1.load_policy(policy_fileName)\n","\n","p2 = SemiRandomPlayer(\"Semi-Random\")\n","p3 = MaxminPlayer(\"Minimax\", max_depth=5)\n","p4 = DQPlayer(\"DQ2\", connect4_dqn_args, isLearning=False)\n","p4.load_policy(policy_fileName)\n","opponents = [p2, p3, p4]\n","nGamesList = [10000, 10, 2]\n","multi_evaluation(state, p1, opponents, nGamesList)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xwgy_r3EMf_8"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"V1IUKa__r8dV"},"source":["---\n","## **Conclusion**\n"]},{"cell_type":"markdown","metadata":{"id":"VkI_PKHwVHsF"},"source":["---\n","## **Let's play!**\n","\n","In this section you can try yourself to play against each of the players:  \n","- ```RandomPlayer(name)``` - A random player.\n","- ```SemiRandomPlayer(name)``` - A semi-random player, which acts randomly unless given the opportunity to win the game in the next turn.\n","- ```MaxminPlayer(name, depth)``` - A minimax player which checks `depth` steps forward and uses some heuristic to take the best resulting action. For `depth = 5` this player will be a very good opponent (from my experience playing against it).\n","- ```DQPlayer(name, connect4_dqn_args, isLearning=False)``` - Our trained Deep Q-learning player with a policy loaded by `load_policy(policy_fileName)`.\n","- ```HumanPlayer(name)``` - Another human, play against a friend.  \n","\n","Good luck!\n","\n"]},{"cell_type":"code","metadata":{"id":"8MMUJ62DdEzt"},"source":["policy_fileName = \"./Connect4/DQpolicy_vsSemiRand_selfPlay\"\n","p1 = HumanPlayer(\"Player\")\n","p2 = DQPlayer(\"DQ\", connect4_dqn_args, isLearning=False)\n","p2.load_policy(policy_fileName)\n","game = Game(state, p1, p2)\n","game.play()\n","game.reset()"],"execution_count":null,"outputs":[]}]}